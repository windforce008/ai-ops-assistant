# week2_day6_streamlit_web.py  â€”â€” Day6æœ¬åœ°Webç•Œé¢ï¼ˆStreamlitç‰ˆè¿ç»´ChatGPTï¼‰

import streamlit as st
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.chat_models.tongyi import ChatTongyi
from langchain_classic.chains import create_retrieval_chain
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from pathlib import Path

# ========= é…ç½® =========
QIANWEN_API_KEY = "sk-ä½ çš„apikey"
CHROMA_DB_PATH = "D:/chroma_db_multi"  # ç”¨Day1æˆ–Day5å»ºçš„åº“,è¿™é‡Œçš„ç›®å½•éœ€è¦ä½ åˆ›å»º
# ===========================

# é¡µé¢é…ç½®
st.set_page_config(page_title="è¿ç»´AIåŠ©æ‰‹", page_icon="ğŸ¤–", layout="wide")
st.title("ğŸ¤– è¿ç»´AIåŠ©æ‰‹ - å…¬å¸ITä¸“å±çŸ¥è¯†åº“")
st.caption("åŸºäºå…¬å¸æ–‡æ¡£çš„RAGé—®ç­”ç³»ç»Ÿï¼Œç”±æå…ˆç”Ÿæ‰“é€ ")

# ä¾§è¾¹æ 
with st.sidebar:
    st.header("é…ç½®")
    st.write(f"å‘é‡åº“è·¯å¾„: {CHROMA_DB_PATH}")
    st.write("æ¨¡å‹: é€šä¹‰åƒé—® qwen-turbo")
    st.divider()
    st.caption("è¾“å…¥é—®é¢˜åå›è½¦å‘é€ï¼Œæ”¯æŒå¤šè½®å¯¹è¯")

# åˆå§‹åŒ–
@st.cache_resource
def load_rag_chain():
    embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-large-zh-v1.5")
    vectorstore = Chroma(persist_directory=CHROMA_DB_PATH, embedding_function=embeddings)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

    llm = ChatTongyi(model="qwen-turbo", dashscope_api_key=QIANWEN_API_KEY)

    qa_prompt = ChatPromptTemplate.from_template(
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¿ç»´åŠ©æ‰‹ã€‚æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼Œç”¨ä¸­æ–‡ï¼Œç®€æ´ä¸“ä¸šï¼š\n\n{context}\n\né—®é¢˜ï¼š{input}"
    )
    question_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(retriever, question_chain)
    return rag_chain

rag_chain = load_rag_chain()

# èŠå¤©å†å²
if "messages" not in st.session_state:
    st.session_state.messages = []

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "sources" in message:
            with st.expander("æ¥æºæ–‡æ¡£"):
                st.write(message["sources"])

# ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("é—®æˆ‘è¿ç»´é—®é¢˜..."):
    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    st.chat_message("user").write(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # è°ƒç”¨RAG
    with st.chat_message("assistant"):
        with st.spinner("æ€è€ƒä¸­..."):
            result = rag_chain.invoke({"input": prompt})
            answer = result["answer"]

            # æ¥æº
            sources = ""
            seen = set()
            for doc in result.get("context", [])[:3]:
                filename = Path(doc.metadata.get('source', 'æœªçŸ¥æ–‡ä»¶')).name
                if filename not in seen:
                    sources += f"- {filename}\n"
                    seen.add(filename)

            full_answer = answer
            if sources:
                full_answer += f"\n\n**æ¥æºæ–‡æ¡£ï¼š**\n{sources}"

            st.markdown(full_answer)

    # ä¿å­˜AIæ¶ˆæ¯
    st.session_state.messages.append({
        "role": "assistant",
        "content": full_answer,
        "sources": sources
    })

# åº•éƒ¨
st.divider()
st.caption("çƒ­çƒˆåº†ç¥å…¬å¸ä¸“å±è¿ç»´ChatGPTå·²ä¸Šçº¿")
